# CXR_Embedding_Fairness , Underdiagnosis bias mitigation with expert foundation model's representation

## Dataset access:
Both MIMIC-CXR, CheXpert datasets in image and vector embedding forms used for this work are public under data use agreements. 

MIMIC-CXR image  dataset is available at: https://physionet.org/content/mimic-cxr/2.0.0/

MIMIC-CXR embeddings  dataset is available at: https://physionet.org/content/image-embeddings-mimic-cxr/1.0/

CheXpert image dataset is available at: https://stanfordmlgroup.github.io/competitions/chexpert/

CheXpert embeddings dataset can be requested by filling out the [CXR Foundation Access Form](https://docs.google.com/forms/d/e/1FAIpQLSek0P-JSwSfonIiZJlz7gOTbL0lugsDug0FUnMhS1zVzpEKlg/viewform) from the authors of the CXR foundation model.

Access to all three datasets requires user registration and the signing of a data use agreement. Only the MIMIC-CXR dataset requires the completion of an additional credentialing process. After following these procedures, the MIMIC-CXR data is available through PhysioNet (https://physionet.org/). The race/ethnicities and insurance type of the patients are not provided directly with the download of the MIMIC-CXR dataset. However, this data is available through merging the patient IDs in MIMIC-CXR with subject IDs in MIMIC-IV (https://physionet.org/content/mimiciv/0.4/) datasets, using the patient and admissions tables. Access to MIMIC-IV requires a similar procedure as MIMIC-CXR and the same credentialing process is applicable for both datasets. 

----------------------------------------------------------------------------------------------------------------------------
## Reproducing the results:

### Model Configuration : CXR_Emb

Model Configuration files for the CXR_Emb model are located in the ./configs directory under CXR_Emb/Training and evalaution  folder. 

#### Training
To train the model, use the following command. Replace ***.yaml with the name of your specific configuration file, and execute for different seeds. The seed numbers we used are [19,31,38,47,77]

`python main.py fit -c ./configs/***_config.yaml `

#### Testing
After training, the model can be tested  by specifying the path to the saved checkpoint and the configuration file used for training:

`python main.py test -c ./configs/***_config.yaml --ckpt_path ./path_to_saved_checkpoint `

#### Underdiagnosis analysis

Underdiagnosis analysis for the `CXR_Emb` dataset is available in the `CXR_Emb/Fairness` directory. This directory contains five separate folders, each corresponding to a different dataset forms . For each scenario, we calculate the False Positive Rate (FPR) across different random seeds, compute the average FPR over five runs with 95% confidence intervals, and visualize the results using the `FPR_Visualizations_**.ipynb` notebook. Replace `**` with the appropriate dataset name (e.g., `MIMIC` for the MIMIC dataset).

### Model Configuration : BMC_Emb
Model Configuration files for the CXR_Emb model are located in the ./configs directory. 

## Training and testing 

Training and testing for this scenario are located in the `**_Biomedclip.py` scripts under the `BMC_Emb` directory. You will find three folders corresponding to the `CheXpert`, `MIMIC`, and `ALL` datasets. Replace `**` with the appropriate dataset name (e.g., `MIMIC` for the MIMIC dataset).


#### Underdiagnosis analysis

Underdiagnosis analysis for the `BMC_Emb` dataset are available under the three folders of the `BMC_Emb` directory. For each scenario, we calculate the False Positive Rate (FPR) across different random seeds, compute the average FPR over five runs with 95% confidence intervals, and visualize the results using the `FPR_Visualizations_**.ipynb` notebook. Replace `**` with the appropriate dataset name (e.g., `MIMIC` for the MIMIC dataset).


 We are not able to share the trained model and the true label and predicted label CSV files of the test set due to the data-sharing agreement. However, we have provided the random seed, and the code. Then, the true label and predicted label CSV files and trained models can be generated by users who have downloaded the data from the original source following the procedure that is described in the “Data access” session.
